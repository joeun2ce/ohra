# SageMaker serving configuration for Qwen3-embedding model
# This file configures the model server for deployment on AWS SageMaker

# Use Python inference engine
engine=Python
option.entryPoint=inference

# HuggingFace model identifier
# Model will be automatically downloaded from HuggingFace
option.model_id=Qwen/Qwen3-Embedding-0.6B-GGUF:Qwen3-Embedding-0.6B-Q8_0.gguf

# Model configuration

# Maximum input text length (in tokens)
# Inputs longer than this will be truncated
# Higher values allow longer documents but use more memory
# Recommended: 1024 for most use cases, up to 4096 for long documents
option.max_length=1024

# Context window size (in tokens)
# Maximum sequence length the model can process in one pass
# Must be >= max_length
# Higher values use more GPU/CPU memory
# Qwen3-Embedding-4B supports up to 32K, but 1024 is sufficient for most cases
option.n_ctx=1024

# Number of CPU threads for inference
# More threads = faster processing but higher CPU usage
# Recommended: 4-8 for most instances
# Set to number of vCPUs for maximum throughput
option.n_threads=4

# Number of layers to offload to GPU
# -1 = auto (offload all layers to GPU if available, otherwise CPU)
# 0 = CPU only
# >0 = specific number of layers to offload
option.n_gpu_layers=-1